





import ddsapi
import requests
import datetime
import time


c = ddsapi.Client(key="2bfcbf76-ce9f-4ef5-8853-fdbac42ec90e:YIKkXeWqChOn7U-2bcvCGvqy9ocdsjiH1-cv-2NvvUE")

c.retrieve("era5-downscaled-over-italy", "hourly",
{
    "area": {
        "north": 45.71748,
        "south": 45.19620,
        "east": 9.67184,
        "west": 8.67171
    },
    "time": {
        "hour": [
            "00",
            "01",
            "02",
            "03",
            "04",
            "05",
            "06",
            "07",
            "08",
            "09",
            "10",
            "11",
            "12",
            "13",
            "14",
            "15",
            "16",
            "17",
            "18",
            "19",
            "20",
            "21",
            "22",
            "23"
        ],
        "year": [
            "1981",
            "1982",
            "1983",
            "1984",
            "1985",
            "1986",
            "1987",
            "1988",
            "1989",
            "1990",
            "1991",
            "1992",
            "1993",
            "1994",
            "1995",
            "1996",
            "1997",
            "1998",
            "1999",
            "2000",
            "2001",
            "2002",
            "2003",
            "2004",
            "2005",
            "2006",
            "2007",
            "2008",
            "2009",
            "2010",
            "2011",
            "2012",
            "2013",
            "2014",
            "2015",
            "2016",
            "2017",
            "2018",
            "2019",
            "2020",
            "2021",
            "2022",
            "2023"
        ],
        "month": [
            "1",
            "2",
            "3",
            "4",
            "5",
            "6",
            "7",
            "8",
            "9",
            "10",
            "11",
            "12"
        ],
        "day": [
            "1",
            "2",
            "3",
            "4",
            "5",
            "6",
            "7",
            "8",
            "9",
            "10",
            "11",
            "12",
            "13",
            "14",
            "15",
            "16",
            "17",
            "18",
            "19",
            "20",
            "21",
            "22",
            "23",
            "24",
            "25",
            "26",
            "27",
            "28",
            "29",
            "30",
            "31"
        ]
    },
    "variable": [
        "air_temperature"
    ],
    "format": "netcdf"
},
"era5-downscaled-over-italy-hourly-CML_ROI.nc")





import numpy as np                      
from matplotlib import pyplot as plt   
import matplotlib.dates as mdates
import matplotlib.cm as cm
import matplotlib.colors as mcolors
import xarray as xr                       # for netcdf
import pandas as pd   
import geopandas as gpd
import os
import rasterio
import rioxarray as rio
from rasterio.transform import from_origin
from rasterio.crs import CRS
from scipy.interpolate import griddata
from shapely.geometry import Point





file_path = ".//Data//era5-downscaled-over-italy-CML.nc"
ds = xr.open_dataset(file_path)
ds.lon


# We are interested in the air temperature variable, but we will convert Kelvins to °C. New variable is 'ds_c'.
ds.time
ds_c = ds.assign(T_2M=ds.T_2M - 273.15)
print(ds_c.lon)








# Just a note that each lat and lon coordinate have rlat and rlon variable), which allows lat/lon extraction along x and y
ds_final = ds_c.assign_coords(
    lon=("rlon", ds_c.lon.isel(rlat=0).values),  # Take first row of lon
    lat=("rlat", ds_c.lat.isel(rlon=0).values)   # Take first column of lat
)

# Swap dimensions: Replace rlon/rlat with lon/lat
ds_final = ds_final.swap_dims({"rlon": "lon", "rlat": "lat"})
ds_final


# Let's just check if the first column/row or rlat and rlon coincides with the first column/row of lat/lon. We can compare it back to the original nc file upper in the code.
print(f"First row of lon: {ds_c.lon.isel(rlat=0).values}")
print(f"First column of lat: {ds_c.lat.isel(rlon=0).values}")
print(f"rlon values: {ds_c.rlon.values}")
print(f"rlat values: {ds_c.rlat.values}")


# Plot just one time step to check if pixels look good
data = ds_final["T_2M"].isel(time=349044)

plt.figure(figsize=(8, 6))
data.plot(cmap="RdYlGn_r", x="lon", y="lat")
plt.show()


# Here we try to export to gpkg with the idea to convert to raster

# Load dataset
file_path = ".//Data//era5-downscaled-over-italy-CML.nc"
ds = xr.open_dataset(file_path)

# Ensure time is decoded as datetime
ds["time"] = xr.decode_cf(ds).time

# Select July 15–26, hours 16 and 17
time_filtered = ds.sel(
    time=((ds["time"].dt.month == 6) &
          (ds["time"].dt.day >= 11) &
          (ds["time"].dt.day <= 21) &
          (ds["time"].dt.hour.isin([5, 6])))
)

# Average over time and convert from Kelvin to Celsius
t2m_mean = time_filtered["T_2M"].mean(dim="time") - 273.15

# Get coordinate arrays
lat = ds["lat"].values
lon = ds["lon"].values

# Meshgrid lat/lon if needed
if lat.ndim == 1 and lon.ndim == 1:
    lon, lat = np.meshgrid(lon, lat)

# Flatten arrays
flat_temp = t2m_mean.values.flatten()
flat_lat = lat.flatten()
flat_lon = lon.flatten()

# Remove NaNs (optional but often necessary)
mask = ~np.isnan(flat_temp)
flat_temp = flat_temp[mask]
flat_lat = flat_lat[mask]
flat_lon = flat_lon[mask]

# Create GeoDataFrame
df = pd.DataFrame({"temperature": flat_temp})
geometry = [Point(xy) for xy in zip(flat_lon, flat_lat)]
gdf = gpd.GeoDataFrame(df, geometry=geometry, crs="EPSG:4326")

# Visualization
plt.figure(figsize=(10, 8))
gdf.plot(column="temperature", cmap="RdYlBu_r", legend=True, markersize=20)
plt.title("Avg 2m Air Temperature (°C), MUHI")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.grid(True)
plt.tight_layout()
plt.show()

# Export to GeoPackage
gdf.to_file("t2m_mean_june11_21_hour5_6.gpkg", driver="GPKG")


print(t2m_interp.rio.crs)


# Selecting time step
first_band = ds_final.isel(time=5000)  # Extract the first time step

# variable to export
data_var = first_band["T_2M"]

# Ensuring georeferencing information
data_var.rio.set_spatial_dims(x_dim="lon", y_dim="lat", inplace=True)
data_var.rio.write_crs("EPSG:4326", inplace=True)  # Set projection (change if needed)

data_var.rio.to_raster(".//outputs//try6.tif")








# Clip 2022 only
ds_2022 = ds_c.sel(time=slice("2022-01-01", "2022-12-31"))

# Clip full period for calculating long-term threshold
ds_long = ds_c.sel(time=slice("1981-01-01", "2021-12-31"))

# Clip period that overlaps with ARPA stations (1989-2021) for calculating long-term threshold
ds_arpa_overlap = ds_c.sel(time=slice("1989-01-01", "2021-12-31"))


# Here we calculate long-term daily maximum temperature thresholds. time=1D indicates a time step of one day.
daily_max = ds_long['T_2M'].resample(time='1D').max()

# Full dataset should have 14975 days between 01/01/1981 and 31/12/2021. Let's check if there are some missing days
print(daily_max['time'])


# Apply rolling 15-day centered window
rolling_window_15 = daily_max.rolling(time=15, center=True, min_periods=1)
#rolling_window_31 = daily_max.rolling(time=31, center=True, min_periods=1)


# Compute 90th percentile over the entire time period (per pixel)
daily_max_90p_rolling_15 = (
    rolling_window_15.construct("window")
    .reduce(np.percentile, q=90, dim="window")
)

# daily_max_90p_rolling_31 = (
#     rolling_window_31.construct("window")
#     .reduce(np.percentile, q=90, dim="window")
# )


# Collapse per day
daily_max_90p_clim_15 = daily_max_90p_rolling_15.groupby('time.dayofyear').mean('time')
#daily_max_90p_clim_31 = daily_max_90p_rolling_31.groupby('time.dayofyear').mean('time')


daily_max_90p_clim_15


# Plot thresholds

# Convert to DataFrame
df_15 = daily_max_90p_clim_15.to_dataframe(name='threshold_15').reset_index()

#df_31 = daily_max_90p_clim_31.to_dataframe(name='threshold_31').reset_index()

# Check columns to ensure 'dayofyear' exists
print(df_15.columns)

# Plot the data directly without merging
plt.figure(figsize=(12, 6))

# Plot the 15-day threshold
plt.plot(
    df_15['dayofyear'],
    df_15['threshold_15'],
    label="Threshold in °C (15-day window)",
    alpha=0.8,
    linewidth=2.0
)

# Plot the 31-day threshold
# plt.plot(
#     df_31['dayofyear'],
#     df_31['threshold_31'],
#     label="Threshold in °C (31-day window)",
#     color="red",
#     alpha=0.4,
#     linewidth=1
# )

# Labeling the axes and title
plt.xlabel("Day of the year")
plt.ylabel("Temperature (°C)")
plt.title("Thresholds for Heatwaves")

# Adding the legend
plt.legend()

# Enabling grid
plt.grid(axis='both', linestyle='--', alpha=0.7)

# Set x-ticks based on day of year and label them with month names
unique_months = df_15.drop_duplicates(subset="dayofyear")

# Save the figure before showing it
plt.savefig("heatwave_thresholds_15d-window.png", dpi=600, bbox_inches='tight')

# Show the plot
plt.show()





# Let's quickly visualise it
plt.figure(figsize=(8, 6))
daily_max_90p_clim_15.isel(dayofyear=365).plot(cmap="RdYlGn_r", x="lon", y="lat")
plt.show()


# Export thresholds to nc file
daily_max_90p_clim_15.to_netcdf(".//outputs//Daily_Thresholds_90p_15_long.nc")








# Compute maximum daily temperature for 2022
daily_max_2022 = ds_2022["T_2M"].resample(time="1D").max()
daily_max_2022.to_netcdf(".//outputs//max_daily_temp_2022.nc")
daily_max_2022


# Align percentile dataset by day of year
threshold_2022 = daily_max_90p_clim_15.sel(dayofyear=daily_max_2022["time"].dt.dayofyear)


# Create HW boolean mask (True if daily max > threshold)
heatwave_mask = daily_max_2022 > threshold_2022
#heatwave_mask.to_netcdf(".//outputs//heatwave_boolean_2022.nc")


# Let's see the number of HW days per pixel
hw_days_per_pixel = heatwave_mask.sum(dim="time")
hw_days_per_pixel





# Calculate the majority HW occurrence per day (if >90% pixels are True, mark as True)
hw_majority = heatwave_mask.mean(dim=["rlat", "rlon"]) > 0.90
hw_majority = hw_majority.to_dataframe(name="heatwave_occurrence").reset_index()


print(hw_majority.to_string())


print(hw_majority["heatwave_occurrence"].value_counts())


hw_majority.to_csv(".//outputs//hw_majority_CMCC_ARPA_overlap_15d.csv")





diff_intensity  = daily_max_2022 - threshold_2022


mean_intensity = diff_intensity.mean(dim=['rlat', 'rlon'], skipna=True)


mean_intensity = mean_intensity.to_dataframe(name="HW_intensity").reset_index()


mean_intensity


# Merge HW occurrence & intensity into one df
merged_HW = pd.merge(mean_intensity, hw_majority, on='time')[['time','HW_intensity', 'heatwave_occurrence']]


merged_HW.head(60)


# Assuming merged_HW DataFrame is already defined with 'time', 'HW_intensity', 'heatwave_occurrence'
merged_HW['time'] = pd.to_datetime(merged_HW['time'])

# Step 1: Identify consecutive heatwave days
merged_HW['heatwave_group'] = (merged_HW['heatwave_occurrence'] != merged_HW['heatwave_occurrence'].shift()).cumsum()

# Step 2: Filter only the heatwaves (where heatwave_occurrence is True)
heatwaves = merged_HW[merged_HW['heatwave_occurrence']]

# Step 3: Group by heatwave and filter those that are at least 3 days long
heatwave_groups = heatwaves.groupby('heatwave_group').filter(lambda group: len(group) >= 3)

# Step 4: Calculate the start and end date of each heatwave, the number of days, and cumulative and average intensity
heatwave_summary = heatwave_groups.groupby('heatwave_group').agg(
    start_date=('time', 'min'),
    end_date=('time', 'max'),
    num_days=('time', 'size'),
    cumulative_intensity=('HW_intensity', 'sum'),
    average_intensity=('HW_intensity', 'mean')
).reset_index()

# Result
print(heatwave_summary)



# Normalize the average intensity for colormap scaling
norm = mcolors.Normalize(vmin=heatwave_summary['average_intensity'].min(), vmax=heatwave_summary['average_intensity'].max())
cmap = cm.viridis 
sm = cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([]) 

fig, ax = plt.subplots(figsize=(6, 4))

# Plot each heatwave with a color corresponding to its average intensity
for idx, row in heatwave_summary.iterrows():
    color = sm.to_rgba(row['average_intensity'])
    ax.plot([row['start_date'], row['end_date']], [idx, idx], color=color, lw=6, label='Heatwave' if idx == 0 else "")

# Add colorbar for intensity scale
fig.colorbar(sm, ax=ax, label='Average Intensity (°C)')

#ax.set_title('Heatwaves during 2022', fontsize=12)
ax.set_xlabel('Date', fontsize=11)
ax.set_xlim(pd.Timestamp('2022-01'), pd.Timestamp('2023-01'))
ax.set_ylabel('HW event number', fontsize=11)
ax.set_ylim(1,17)

ax.xaxis.set_major_locator(mdates.MonthLocator())
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))
plt.xticks(rotation=45)

ax.grid(True, axis='x', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.savefig('heatwaves_2022.png', dpi=600)
plt.show()


import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.cm as cm
import matplotlib.colors as mcolors
import pandas as pd

# Normalize the average intensity for colormap scaling
norm = mcolors.Normalize(
    vmin=heatwave_summary['average_intensity'].min(),
    vmax=heatwave_summary['average_intensity'].max()
)
cmap = cm.viridis
sm = cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])

fig, ax = plt.subplots(figsize=(6, 4))

# Plot each heatwave
for idx, row in heatwave_summary.iterrows():
    color = sm.to_rgba(row['average_intensity'])
    ax.plot([row['start_date'], row['end_date']], [idx + 1, idx + 1], color=color, lw=6)

# Colorbar
fig.colorbar(sm, ax=ax, label='Average intensity (°C)')

# Labels and limits
ax.set_xlabel('Date', fontsize=11)
ax.set_xlim(pd.Timestamp('2022-01-01'), pd.Timestamp('2023-01-01'))
ax.set_ylabel('HW event number', fontsize=11)
ax.set_ylim(0.5, 16.5)

# Show only month names
ax.xaxis.set_major_locator(mdates.MonthLocator())
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))

plt.xticks(rotation=0)
ax.grid(True, axis='x', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.savefig('heatwaves_2022.png', dpi=600)
plt.show()



